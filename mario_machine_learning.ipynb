{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3319745f-b1c0-4218-9269-52e79f8461dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group Number: 27\n",
    "#Group Members: Luca Macesanu, Samik Singh, Carson Ngyuen, Mathew Huang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61887bd6-01fc-4527-a964-ef48e9a1a6c6",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "For our project, we wanted to create a model to play a videogame. During the process, we first considered playing pokemon. However, we were unable to find a robust dataset to use. Therefore, we ended up moving forward with the gym retro library where we were able to load games and create our own dataset. The game we wanted to choose was Mortal Kombat. However, that didn't work as we were unable to get it to run. Therefore, we settled on Super Mario Bros. Therefore, we decided to create models that would be able to complete world 1-1 of Super Mario Bros.\n",
    "\n",
    "\n",
    "We first collected data. Then we moved on to using classification based approaches similar to what we learned in class. Lastly, we used Proximal Policy Optimization to expand upon our knowledge. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99993be6-415c-4842-8c57-e7a5494846b8",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "\n",
    "Since there were no existing datasets for our project, we decided to collect our own data. We collected a total of 128942 state action pairs in order to treat gameplay as a classification problem in order to predict how a human would play the game. \n",
    "\n",
    "\n",
    "Since we collected the data, data cleaning was more minimal. We remapped our action spaces as there were many action outputs that matched the state of the game. By consolidating these actions, we reduced the overall spread of our data. We also scaled the state space by normalizing pixel values making it easier to process as well. Lastly, we manually combed through the files removing corrupted or improperly recorded data.\n",
    "\n",
    "### Refer to collect_data.py to test out data collection script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a205298f-5ec3-42a9-8112-b4d04459bdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Custom imports\n",
    "from playback import get_state_action_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6e364dc-8c03-45c3-90af-32511e2a0c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminate_unnecessary_action(data):\n",
    "    action_space = data\n",
    "    \n",
    "    #This might not be necessary but im on a time crunch (Theres 100% a better way to do this)\n",
    "    action_map = {\n",
    "        64: 0,\n",
    "        65: 1,\n",
    "        66: 2,\n",
    "        67: 3,\n",
    "        128: 4,\n",
    "        129: 5,\n",
    "        130: 6,\n",
    "        131: 7\n",
    "    }\n",
    "    \n",
    "    # Map original action values to new values\n",
    "    action_space = np.array([action_map.get(action, 0) for action in action_space])\n",
    "    #print(action_space.size())\n",
    "    \n",
    "    return action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987e87a4-acee-446b-8abe-e385384c20de",
   "metadata": {},
   "source": [
    "We generated graphs to display the frequency and the average state of a subset of our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e86c9e4-388b-4587-93ca-0943e0fcd54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding  imitation_mario_rec_carson_032124_142721.npz\n",
      "Adding  imitation_mario_rec_carson_032124_143046.npz\n",
      "Adding  imitation_mario_rec_carson_032124_143622.npz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Action Frequency Chart')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generate bar chart of action frequencies\n",
    "rec_state_history, rec_action_history = get_state_action_pairs(3)\n",
    "#load data\n",
    "rec_action_history = eliminate_unnecessary_action(rec_action_history)\n",
    "\n",
    "unique_vals, frequencies = np. unique(rec_action_history, return_counts = True)\n",
    "plt.bar(unique_vals, frequencies)\n",
    "plt.xlabel('Action')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Action Frequency Chart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1dc066d-3d80-4241-8d37-119f12ec3805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_images(images):\n",
    "    average_image = np.mean(images, axis=0)\n",
    "    plt.imshow(average_image, cmap = 'gray')\n",
    "    plt. title('Average Image')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d62e5e8-dd96-4105-9d2c-0d06f7be7823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#np mean of states\n",
    "average_images(rec_state_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00380ee-e1f5-4933-b477-aa22beae37e9",
   "metadata": {},
   "source": [
    "# Classification Based Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778c7809-0a65-4767-a59e-24a139de2bd8",
   "metadata": {},
   "source": [
    "## 1. KNN\n",
    "\n",
    "We used KNN as an example as to why our agent should not use classification methods using state-action pairs to solve the issue. One issue is KNN's susceptibility to noise. Since there are so many permutations to how a scene could be set depending on where the enemies are relative to the scene, it is hard for us to fully clean our data of noises as it is a large part of the game. \n",
    "\n",
    "Another issue for KNN is the curse of dimensionality. This dataset is composed of two numpy arrays: a 3D array that describes the environment and a 1D array that describes the action taken. In many cases in training, our players did drastically different actions at the same observation or the same action at a slightly different observation. Therefore, KNN has difficulties in finding similarities between records. This resulted in our KNN model taking vast amounts of time for training off of a singular isolated run and thus not finishing. We included the code as a large comment since it wil be unable to run in a realistic timeframe due to the sheer size of our data, even when drastically reducing the amount of data used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36b321f5-20c2-4598-b877-8324f1296921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport gym\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\\nimport pickle\\nfrom datetime import datetime\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom tqdm import tqdm\\nfrom gym.wrappers import GrayScaleObservation\\nimport os\\nimport sklearn as sk\\nfrom sklearn.pipeline import Pipeline\\n\\n# Custom imports assuming these are correctly implemented\\nfrom playback import get_state_action_pairs\\n\\nclass knn:\\n    def __init__(self, model = None, n_n= 10):\\n        self.done = False\\n        self.name = \"KNN\"\\n\\n        #generates a pipeline that scales and decreases dimensions. \\n        if model is None:\\n            pline = Pipeline([(\\'scaler\\', sk.preprocessing.StandardScaler()), (\\'pca\\', sk.decomposition.PCA()), \\n                  (\\'knn\\', KNeighborsClassifier(n_neighbors= n_n))])\\n            self.model = pline\\n        else:\\n            self.model = model\\n\\n    def get_action(self, state):\\n        state = state.flatten().reshape(1, -1)\\n        return self.model.predict(state)\\n\\n    def train(self, state_history, action_history):\\n        self.model.fit(state_history, action_history)\\n        pickle.dump(self.model, open(\"knn.p\", \"wb\"))\\n        print(\"Training Complete.\")\\n\\n    def evaluate_and_tune(self, features, labels):\\n        parameters = {\\n             \\'n_neighbors\\': [None, 5, 10, 15]\\n        }\\n        cv = 5\\n        grid_search = GridSearchCV(self.model, parameters, cv=cv, scoring=\\'accuracy\\', return_train_score=True)\\n        grid_search.fit(features, labels)\\n\\n        print(\\'Best Parameters:\\', grid_search.best_params_)\\n        return grid_search\\n\\ndef plot_accuracy(grid_search):\\n    results = grid_search.cv_results_\\n    plt.figure(figsize=(10, 5))\\n    plt.title(\"Training vs Validation Accuracy\")\\n    plt.plot(results[\\'mean_train_score\\'], label=\\'Train Accuracy\\')\\n    plt.plot(results[\\'mean_test_score\\'], label=\\'Validation Accuracy\\')\\n    plt.xlabel(\\'Parameter Combination\\')\\n    plt.ylabel(\\'Accuracy\\')\\n    plt.legend()\\n    plt.show()\\n\\ndef run_agent(agent):\\n    env = GrayScaleObservation(gym.make(\"SuperMarioBros-v3\"))\\n    env.reset()\\n    done = False\\n    state_history = []\\n    action_history = []\\n\\n    with tqdm(total=100) as pbar:\\n        while not done:\\n            state, reward, done, _ = env.step(0)  # Example using a constant action\\n            action = agent.get_action(state)\\n            state_history.append(state)\\n            action_history.append(action)\\n            pbar.update(1)\\n\\n    env.close()\\n\\n\\n\\nif __name__ == \"__main__\":\\n    record_path = \"recordings\"\\n    \\n    if not os.path.isdir(record_path):\\n        print(f\"Error: {record_path} is not a valid directory.\")\\n    else:\\n        rec_state_history, rec_action_history = get_state_action_pairs(2)\\n\\n        if rec_state_history.ndim == 3:\\n            nsamples, height, width = rec_state_history.shape\\n            rec_state_history = rec_state_history.reshape((nsamples, height * width))\\n\\n        print(\"Reshaped state history shape:\", rec_state_history.shape)\\n\\n        train_states, test_states, train_actions, test_actions = train_test_split(\\n            rec_state_history, rec_action_history, test_size=0.2, random_state=42)\\n\\n        agent = knn()\\n        agent.train(train_states, train_actions)\\n\\n        grid_search = agent.evaluate_and_tune(train_states, train_actions)\\n        plot_accuracy(grid_search)\\n\\n        run_agent(agent)\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from tqdm import tqdm\n",
    "from gym.wrappers import GrayScaleObservation\n",
    "import os\n",
    "import sklearn as sk\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Custom imports assuming these are correctly implemented\n",
    "from playback import get_state_action_pairs\n",
    "\n",
    "class knn:\n",
    "    def __init__(self, model = None, n_n= 10):\n",
    "        self.done = False\n",
    "        self.name = \"KNN\"\n",
    "\n",
    "        #generates a pipeline that scales and decreases dimensions. \n",
    "        if model is None:\n",
    "            pline = Pipeline([('scaler', sk.preprocessing.StandardScaler()), ('pca', sk.decomposition.PCA()), \n",
    "                  ('knn', KNeighborsClassifier(n_neighbors= n_n))])\n",
    "            self.model = pline\n",
    "        else:\n",
    "            self.model = model\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = state.flatten().reshape(1, -1)\n",
    "        return self.model.predict(state)\n",
    "\n",
    "    def train(self, state_history, action_history):\n",
    "        self.model.fit(state_history, action_history)\n",
    "        pickle.dump(self.model, open(\"knn.p\", \"wb\"))\n",
    "        print(\"Training Complete.\")\n",
    "\n",
    "    def evaluate_and_tune(self, features, labels):\n",
    "        parameters = {\n",
    "             'n_neighbors': [None, 5, 10, 15]\n",
    "        }\n",
    "        cv = 5\n",
    "        grid_search = GridSearchCV(self.model, parameters, cv=cv, scoring='accuracy', return_train_score=True)\n",
    "        grid_search.fit(features, labels)\n",
    "\n",
    "        print('Best Parameters:', grid_search.best_params_)\n",
    "        return grid_search\n",
    "\n",
    "def plot_accuracy(grid_search):\n",
    "    results = grid_search.cv_results_\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title(\"Training vs Validation Accuracy\")\n",
    "    plt.plot(results['mean_train_score'], label='Train Accuracy')\n",
    "    plt.plot(results['mean_test_score'], label='Validation Accuracy')\n",
    "    plt.xlabel('Parameter Combination')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def run_agent(agent):\n",
    "    env = GrayScaleObservation(gym.make(\"SuperMarioBros-v3\"))\n",
    "    env.reset()\n",
    "    done = False\n",
    "    state_history = []\n",
    "    action_history = []\n",
    "\n",
    "    with tqdm(total=100) as pbar:\n",
    "        while not done:\n",
    "            state, reward, done, _ = env.step(0)  # Example using a constant action\n",
    "            action = agent.get_action(state)\n",
    "            state_history.append(state)\n",
    "            action_history.append(action)\n",
    "            pbar.update(1)\n",
    "\n",
    "    env.close()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    record_path = \"recordings\"\n",
    "    \n",
    "    if not os.path.isdir(record_path):\n",
    "        print(f\"Error: {record_path} is not a valid directory.\")\n",
    "    else:\n",
    "        rec_state_history, rec_action_history = get_state_action_pairs(2)\n",
    "\n",
    "        if rec_state_history.ndim == 3:\n",
    "            nsamples, height, width = rec_state_history.shape\n",
    "            rec_state_history = rec_state_history.reshape((nsamples, height * width))\n",
    "\n",
    "        print(\"Reshaped state history shape:\", rec_state_history.shape)\n",
    "\n",
    "        train_states, test_states, train_actions, test_actions = train_test_split(\n",
    "            rec_state_history, rec_action_history, test_size=0.2, random_state=42)\n",
    "\n",
    "        agent = knn()\n",
    "        agent.train(train_states, train_actions)\n",
    "\n",
    "        grid_search = agent.evaluate_and_tune(train_states, train_actions)\n",
    "        plot_accuracy(grid_search)\n",
    "\n",
    "        run_agent(agent)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1fbec4-e486-4699-8fec-79df63948353",
   "metadata": {},
   "source": [
    "# 2. Decision Trees\n",
    "We then decided to approach Decision Trees as another method to classify our state-action pairs.\n",
    "\n",
    "This could be potentially fitting as Mario although could be approached creatively could also have a robotic optimal route. We also debated about the potential downsides to a Decision Tree. We believed that our data is filled with many occurrences of the same observation but slightly different actions in reaction to the same environment. \n",
    "\n",
    "In one run, the same person could jump at a frame earlier and thus not jump at the consequent frame or the same person could jump\n",
    "at the consequent frame and not do anything in the frame prior. This makes it difficult for the agent to draw similarities\n",
    "between runs, making it hard for the tree to fit an action to a specific state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b910ab2-1e6f-4c90-87d1-75cc847911e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports for decision trees\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from gym.wrappers import GrayScaleObservation\n",
    "import os\n",
    "from sklearn import tree\n",
    "# Custom imports\n",
    "from playback import get_state_action_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7861cf0-fd8f-409c-a350-11aa1041a4fa",
   "metadata": {},
   "source": [
    "Here we created the code for the Decision Tree Class. We tested 2 parameters with 3 and 4 values respectively. We tested Gini, Entropy, and Log Loss for Criterion, and we tested None, 10, 20, and 30 for max depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c3193e4-9348-4c2c-9a4b-c1dec751aec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Class\n",
    "class DecisionTree:\n",
    "    def __init__(self, model = None, max_d = 10, dummy = False):\n",
    "        self.done = False\n",
    "        self.name = \"decision_tree\"\n",
    "        if model is None:\n",
    "            self.model = tree.DecisionTreeClassifier(criterion='entropy', max_depth=max_d)\n",
    "        else:\n",
    "            self.model = model\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = state.flatten().reshape(1, -1)\n",
    "        return self.model.predict(state)\n",
    "\n",
    "    def train(self, state_history, action_history):\n",
    "        self.model.fit(state_history, action_history)\n",
    "        pickle.dump(self.model, open(\"decision_tree_model.p\", \"wb\"))\n",
    "        print(\"Training Complete.\")\n",
    "\n",
    "    def evaluate_and_tune(self, features, labels):\n",
    "        parameters = {\n",
    "            'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "            'max_depth': [None, 10, 20, 30]\n",
    "        }\n",
    "        cv = 5\n",
    "        grid_search = GridSearchCV(self.model, parameters, cv=cv, scoring='accuracy', return_train_score=True)\n",
    "        grid_search.fit(features, labels)\n",
    "\n",
    "        print('Best Parameters:', grid_search.best_params_)\n",
    "        return grid_search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "574bcedd-7a81-462a-8454-5760530a9631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(agent):\n",
    "    env = GrayScaleObservation(gym.make(\"SuperMarioBros-v3\"))\n",
    "    env.reset()\n",
    "    done = False\n",
    "    state_history = []\n",
    "    action_history = []\n",
    "\n",
    "    with tqdm(total=100) as pbar:\n",
    "        while not done:\n",
    "            state, reward, done, _ = env.step(0)  # Example using a constant action\n",
    "            action = agent.get_action(state)\n",
    "            state_history.append(state)\n",
    "            action_history.append(action)\n",
    "            pbar.update(1)\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd44995d-c13a-4604-bfee-99492f526c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding  imitation_mario_rec_carson_032124_142721.npz\n",
      "Adding  imitation_mario_rec_carson_032124_143046.npz\n",
      "Reshaped state history shape: (13199, 61440)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m train_states, test_states, train_actions, test_actions \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[0;32m     15\u001b[0m     rec_state_history, rec_action_history, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     17\u001b[0m agent \u001b[38;5;241m=\u001b[39m DecisionTree()\n\u001b[1;32m---> 18\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mevaluate_and_tune(train_states, train_actions)\n",
      "Cell \u001b[1;32mIn[9], line 16\u001b[0m, in \u001b[0;36mDecisionTree.train\u001b[1;34m(self, state_history, action_history)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, state_history, action_history):\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_history\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_history\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecision_tree_model.p\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lucam\\miniconda3\\envs\\yumouwei\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lucam\\miniconda3\\envs\\yumouwei\\lib\\site-packages\\sklearn\\tree\\_classes.py:959\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    930\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    931\u001b[0m \n\u001b[0;32m    932\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 959\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\lucam\\miniconda3\\envs\\yumouwei\\lib\\site-packages\\sklearn\\tree\\_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    434\u001b[0m         splitter,\n\u001b[0;32m    435\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[1;32m--> 443\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "record_path = \"recordings\"\n",
    "\n",
    "if not os.path.isdir(record_path):\n",
    "    print(f\"Error: {record_path} is not a valid directory.\")\n",
    "else:\n",
    "    rec_state_history, rec_action_history = get_state_action_pairs(2)\n",
    "\n",
    "    if rec_state_history.ndim == 3:\n",
    "        nsamples, height, width = rec_state_history.shape\n",
    "        rec_state_history = rec_state_history.reshape((nsamples, height * width))\n",
    "\n",
    "    print(\"Reshaped state history shape:\", rec_state_history.shape)\n",
    "\n",
    "    train_states, test_states, train_actions, test_actions = train_test_split(\n",
    "        rec_state_history, rec_action_history, test_size=0.2, random_state=42)\n",
    "\n",
    "    agent = DecisionTree()\n",
    "    agent.train(train_states, train_actions)\n",
    "\n",
    "    grid_search = agent.evaluate_and_tune(train_states, train_actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab10e314-1ceb-4b96-a64a-dd85874d8339",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_agent(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3d7e05-7f63-4025-b21b-c0d7847a18d9",
   "metadata": {},
   "source": [
    "We then visualized our results by comparing the various hyperparameters in their training and validation accuracies. From our results, we noticed that set 1 and 5 had similar training and validation accuracies of roughly 1 and 0.8 respectively. Scikit learn approaches accuracy by using existing data rather than testing it in the Mario Environment in the Gym library. This means that the Decision Tree was good at predicting what a human operator would do. Therefore, this would work great if the data was made by a world class Mario Speedrunner. However, since it isn't and we are not very good at Mario, the decision Tree will not perform at high levels and outperform us by far if at all. \n",
    "\n",
    "\n",
    "Combinations (criterion, max depth):\n",
    "\n",
    "Set 1: Gini + none      \n",
    "\n",
    "Set 2: Gini + 10      \n",
    "\n",
    "Set 3: Gini + 20 \n",
    "\n",
    "Set 4: Gini+ 30 \n",
    "\n",
    "Set 5: Entropy + none \n",
    "\n",
    "Set 6: Entropy + 10 \n",
    "\n",
    "Set 7: Entropy + 20 \n",
    "\n",
    "Set 8: Entropy + 30 \n",
    "\n",
    "Set 9: Log loss + none \n",
    "\n",
    "Set 10: Log loss + 10 \n",
    "\n",
    "Set 11: Log loss + 20 \n",
    "\n",
    "Set 12: Log loss + 30 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce15e28e-6f07-4b0b-b991-c345eb161ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = grid_search.cv_results_\n",
    "sets = ['set 1', 'set 2', 'set 3', 'set 4', 'set 5' , 'set 6', 'set 7', 'set 8', 'set 9', 'set 10', 'set 11', 'set 12']\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.bar(sets, results['mean_train_score'], width= 0.5)\n",
    "plt.xlabel('Parameter Combination')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7a662e-856f-45ad-9637-03086ae646af",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Validation Accuracy\")\n",
    "plt.bar(sets, results['mean_test_score'], width= 0.5)\n",
    "plt.xlabel('Parameter Combination')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4bafbb-8437-4b11-83ce-ab35f25aa9fa",
   "metadata": {},
   "source": [
    "# 3. ADABoosting\n",
    "ADABoosting Description: The next algorithm we utilized to classify our state-action pairs is ADABoosting with decision tree stumps as the base classifier.\n",
    "Though the accuracy of the boosting model is generally higher than techniques alone, for our purposes we should take into account that we are using a supervised learning base model (decision tree stumps). Since the goal of our classification is to classify in a way that allows the model to beat the super mario game, the application of ensembling might not be the best as it would base the action it takes strictly on the states ran through by a mario play-through done by the user, which could be susceptible to faulty playthroughs. If we were to apply an RL agent on the other hand which creates policies on the premise of maximizing rewards as our model for ADABoosting, it would likely be a good approach as the accuracy would be increased substantially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53c5181-42c4-4498-aeb1-d0d7f1abb031",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from nes_py.app.play_human import play_human\n",
    "from gym.wrappers.gray_scale_observation import GrayScaleObservation\n",
    "import cv2\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import *\n",
    "import pickle\n",
    "from playback import get_state_action_pairs\n",
    "from sklearn.model_selection import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ae240f-9510-4184-bcb1-d354609e8ff9",
   "metadata": {},
   "source": [
    "Here we created the code for the Boosting Agent Class. We tested 2 Parameters with 3 different values total. We tested 40, 50, and 60 estimators, and we tested learning rates of 0.8, 1.0, and 1.2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52f55fb-1912-4e83-85e0-e8027aaf9812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADABoost\n",
    "class boosting_agent:\n",
    "    def __init__(self, model = None, n_estimators = 50):\n",
    "        self.done = False\n",
    "        self.name = \"boosting_agent\"\n",
    "        if model is None:\n",
    "            self.model = AdaBoostClassifier(n_estimators = n_estimators)\n",
    "        else:\n",
    "            self.model = model\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = state.flatten().reshape(1, -1)\n",
    "        return self.model.predict(state)\n",
    "\n",
    "    def train(self, state_history, action_history):\n",
    "        self.model.fit(state_history, action_history)\n",
    "        pickle.dump(self.model, open(\"models/booster_model.p\", \"wb\"))\n",
    "        print(\"Training Complete.\")\n",
    "\n",
    "    def evaluate_and_tune(self, features, labels):\n",
    "        parameters = {\n",
    "            'n_estimators': [40, 50, 60],\n",
    "            'learning_rate': [0.8, 1.0, 1.2]\n",
    "        }\n",
    "        cv = 5\n",
    "        grid_search = GridSearchCV(self.model, parameters, cv=cv, scoring='accuracy', return_train_score=True)\n",
    "        grid_search.fit(features, labels)\n",
    "\n",
    "        print('Best Parameters:', grid_search.best_params_)\n",
    "        return grid_search\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cee6c0b0",
   "metadata": {},
   "source": [
    "We begin our run_agent by converting our observations to grayscale then resetting the environment. We then intialize our state_history and action_history and record the said states and actions. We then retrieve actions based on current states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601b3835-413a-41c4-b34f-0a498418a528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(agent):\n",
    "    env = GrayScaleObservation(gym.make(\"SuperMarioBros-v3\"))\n",
    "    env.reset()\n",
    "    done = False\n",
    "    state_history = []\n",
    "    action_history = []\n",
    "\n",
    "    with tqdm(total=100) as pbar:\n",
    "        while not done:\n",
    "            state, reward, done, _ = env.step(0)  # Example using a constant action\n",
    "            action = agent.get_action(state)\n",
    "            state_history.append(state)\n",
    "            action_history.append(action)\n",
    "            pbar.update(1)\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e39eba1",
   "metadata": {},
   "source": [
    "The code below first check wether the record_path exists. It then retrieves the state-action pairs from the recorded data. As specified by the line get_state_action_pairs(2), if our state_history has a three-dimensional shape we then reshape it two-dimensions for compatibility. Finally we allocate 20% of our data as our testing set sie. We then perform evaluation and tuning using grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538de86f-fa07-4f18-8f9a-9b40bec36fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_path = \"recordings\"\n",
    "\n",
    "if not os.path.isdir(record_path):\n",
    "    print(f\"Error: {record_path} is not a valid directory.\")\n",
    "else:\n",
    "    rec_state_history, rec_action_history = get_state_action_pairs(2)\n",
    "\n",
    "    if rec_state_history.ndim == 3:\n",
    "        nsamples, height, width = rec_state_history.shape\n",
    "        rec_state_history = rec_state_history.reshape((nsamples, height * width))\n",
    "\n",
    "    print(\"Reshaped state history shape:\", rec_state_history.shape)\n",
    "\n",
    "    train_states, test_states, train_actions, test_actions = train_test_split(\n",
    "        rec_state_history, rec_action_history, test_size=0.2, random_state=42)\n",
    "\n",
    "    agent = boosting_agent()\n",
    "    agent.train(train_states, train_actions)\n",
    "\n",
    "    grid_search = agent.evaluate_and_tune(train_states, train_actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e60cc4-1365-4eea-b0e5-daa9f9f74eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_agent(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008da409-07fb-44e6-95a0-f4e93dafcfb3",
   "metadata": {},
   "source": [
    "We then visualized our results by comparing the various hyperparameters in their training and validation accuracies. One noticable trend is the overall low training and validation accuracies are all generally low, being close to 0.3 with the highest being from Set 4. This means that it is fitting poorly to the human data. This could potentially mean that it is improving and thus deviating from it. However, more likely than not, this is showing that ADABoosting using a supervised learning base model will perform equal to if not worse than a human operator in Mario.  \n",
    "\n",
    "combinations (n estimators, learning rate):\n",
    "\n",
    "Set 1: (40, 0.8) \n",
    "\n",
    "Set 2: (40, 1.0) \n",
    "\n",
    "Set 3: (40, 1.2) \n",
    "\n",
    "Set 4: (50, 0.8) \n",
    "\n",
    "Set 5 (50, 1.0) \n",
    "\n",
    "Set 6 (50, 1.2) \n",
    "\n",
    "Set 7 (60, 0.8) \n",
    "\n",
    "Set 8(60, 1.0) \n",
    "\n",
    "Set 9 (60, 1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2e14e1-e0c0-4398-a378-2946fa2533c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = grid_search.cv_results_\n",
    "sets = ['set 1', 'set 2', 'set 3', 'set 4', 'set 5' , 'set 6', 'set 7', 'set 8', 'set 9']\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.bar(sets, results['mean_train_score'], width= 0.5)\n",
    "plt.xlabel('Parameter Combination')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ce108d-876b-4300-a0f4-574a724067a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Validation Accuracy\")\n",
    "plt.bar(sets, results['mean_test_score'], width= 0.5)\n",
    "plt.xlabel('Parameter Combination')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38f3207-2ed1-4905-b679-f110329c75fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save boosting agent\n",
    "pickle.dump(agent, open(\"/models/BoostingAgent.p\", \"wb\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9dcd9ab3",
   "metadata": {},
   "source": [
    "# 4. CNN\n",
    "\n",
    "Process: Our process involved splitting the dataset into training and validation sets using a 80-20 ratio. We tested every combination of kernel size and activation function for 4 epochs. Once we decided not to vary our batch size we kept it constant at 32. We then recorded the training histories which included the training and validation accuracies for each combination of hyperparameters.\n",
    "\n",
    "Conclusion: Based on hyperparameter tuning we found that the combination of a kernel size with (7,7) and the sigmoid activation function seemed to be the most effective as evidenced by the training and validation accuracy.\n",
    "\n",
    "Further steps: In order to further tune our hyperparameters to make more accurate predictions we could vary the number of our convolutional layers. Since increasing the number of convolutional layers may allow our model to distinguish between more complex representations, we may be able to improve our performance. We could also test different learning rates to optimize convergence.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8b382e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplayback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_state_action_pairs \n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets, layers, models\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import gym_super_mario_bros\n",
    "import gym\n",
    "import nes_py\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from nes_py.app.play_human import play_human\n",
    "from gym.wrappers.gray_scale_observation import GrayScaleObservation\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import pickle\n",
    "from playback import get_state_action_pairs \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79446eca",
   "metadata": {},
   "source": [
    "The below code converts our game space from rgb to grayscale. Eliminate unecessary action remaps our actionspace from 0->7 as this is what our action input data is looking for. Each number responds to a specific action such as 128 occurs when the user click d and it causes Mario to move forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd15d606",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rgb2gray(rgb):\n",
    "    r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n",
    "    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "    return gray\n",
    "\n",
    "def eliminate_unnecessary_action(data):\n",
    "    action_space = data\n",
    "    action_map = {\n",
    "        64: 0,\n",
    "        65: 1,\n",
    "        66: 2,\n",
    "        67: 3,\n",
    "        128: 4,\n",
    "        129: 5,\n",
    "        130: 6,\n",
    "        131: 7\n",
    "    }\n",
    "    \n",
    "    action_space = np.array([action_map.get(action, 0) for action in action_space])\n",
    "    return action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0275a2",
   "metadata": {},
   "source": [
    "The code below defines a specific CNN architecture. It is using TensorFlow's keras library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c3d16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CNN model with specific layers, some values given from CNN tutorial with tensorflow\n",
    "def create_cnn(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6820097",
   "metadata": {},
   "source": [
    "Kernel Sizes: We tested three different kernel sizes ((3,3),(5,5),(7,7)). As a smaller kernel sizes focus on smaller regions of our input image, we expected the larger kernel sizes to be more effective as they would capture a wider range of information from our input image.\n",
    "\n",
    "Activation Functions: We tested three different activation functions (Rectified Linear Unit, Hyperbolic Tangent, and Sigmoid).\n",
    "As relu (Rectified Linear Unit), is known for its effectiveness with regards to deep learning models, we expected it to perform the best.\n",
    "\n",
    "Note: Tried batch sizes but didn't feel it had a significant enough change in our accuracy. Now we set our batch size to 32.\n",
    "\n",
    "The code below also graphs both the training accuracy and the validation accuracy over a specified amount of epochs. The graph line colors correspond to the specific combination of kernel sizes and and activation functions. The same combination color is used for the training accuracy and the validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a4300b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning(state_history, action_history):\n",
    "    kernel_sizes = [(3, 3), (5, 5),(7,7)]\n",
    "    activation_functions = ['relu','tanh','sigmoid']\n",
    "    #batch_sizes = [32, 64, 128]\n",
    "\n",
    "    training_histories = []\n",
    "    accuracies = []\n",
    "\n",
    "    for kernel_size in kernel_sizes:\n",
    "        for activation_function in activation_functions:\n",
    "            print(f\"training with kernel size: {kernel_size} and activation function: {activation_function}\")\n",
    "            train_a, test_a, train_b, test_b = train_test_split(state_history, action_history, test_size=0.2, random_state=42)\n",
    "            train_a = np.expand_dims(train_a, axis=-1)\n",
    "            test_a = np.expand_dims(test_a, axis=-1)\n",
    "            \n",
    "            train_a = train_a / 255.0\n",
    "            test_a = test_a / 255.0\n",
    "\n",
    "            model = create_cnn(input_shape=(state_history.shape[1], state_history.shape[2], 1), num_classes=np.max(action_history) + 1)\n",
    "            model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "            history = model.fit(train_a, train_b, epochs=20, batch_size=32, validation_data=(test_a, test_b))\n",
    "\n",
    "            training_histories.append(history)\n",
    "            accuracies.append(history.history['val_accuracy'][-1])\n",
    "            plt.figure(figsize=(10, 12))\n",
    "    colors = plt.cm.tab10.colors[:len(kernel_sizes) * len(activation_functions)]\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    for i, kernel_size in enumerate(kernel_sizes):\n",
    "        for j, activation_function in enumerate(activation_functions):\n",
    "            plt.plot(range(1, 21), training_histories[i*len(activation_functions)+j].history['accuracy'], label=f'kernel size:{kernel_size}, activation:{activation_function}')\n",
    "\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('Training Accuracy')\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15),ncol=3,fontsize='small')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    for i, kernel_size in enumerate(kernel_sizes):\n",
    "        for j, activation_function in enumerate(activation_functions):\n",
    "            color = colors[i*len(activation_functions)+j]\n",
    "            plt.plot(range(1, 21), training_histories[i*len(activation_functions)+j].history['val_accuracy'], label=f'kernel size: {kernel_size}, activation:{activation_function}', color=color)\n",
    "\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('validation accuracy')\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=3, fontsize='small')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Validation Accuracies:\")\n",
    "    for i, kernel_size in enumerate(kernel_sizes):\n",
    "        for j, activation_function in enumerate(activation_functions):\n",
    "            print(f'kernel size:{kernel_size}, activation:{activation_function}: {accuracies[i*len(activation_functions)+j]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492c49bf",
   "metadata": {},
   "source": [
    "Define the CNN Agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0de4aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNAgent:\n",
    "    def __init__(self, model=None, state_history=None, action_history=None):\n",
    "        self.done = False\n",
    "        self.name = \"cnn_agent\"\n",
    "        if model is None:\n",
    "            num_classes = np.max(action_history) + 1\n",
    "            self.model = create_cnn(input_shape=(state_history.shape[1], state_history.shape[2], 1), num_classes=num_classes)\n",
    "        else:\n",
    "            self.model = model\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        state = np.expand_dims(state, axis=-1)\n",
    "        return np.argmax(self.model.predict(state))\n",
    "        \n",
    "    def train(self, state_history, action_history):\n",
    "\n",
    "        print(\"Training...\")\n",
    "        train_a, test_a, train_b, test_b = train_test_split(state_history, action_history, test_size=0.2, random_state=42)\n",
    "        train_a = np.expand_dims(train_a, axis=-1)\n",
    "        test_a = np.expand_dims(test_a, axis=-1)\n",
    "        train_a = train_a / 255.0\n",
    "        test_a = test_a / 255.0\n",
    "        self.model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        history = self.model.fit(train_a, train_b, epochs=1, validation_data=(test_a, test_b))\n",
    "        self.model.save(\"models/cnn_model.h5\")\n",
    "        print(\"Training complete.\")\n",
    "\n",
    "        #Graph validation and training accuracy as epochs increase\n",
    "        plt.plot(history.history['accuracy'], label='accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.show()\n",
    "        pass\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5dc3669",
   "metadata": {},
   "source": [
    "Run the CNN Agent and extract the required reward_history, action_history, and state_history from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311b15c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(agent):\n",
    "\n",
    "    env = GrayScaleObservation(gym_super_mario_bros.make(\"SuperMarioBros-v3\"))\n",
    "    env.reset()\n",
    "\n",
    "    action_history = []\n",
    "    state_history = []\n",
    "    reward_history = []\n",
    "\n",
    "    action = [0]\n",
    "    done = False\n",
    "    while not done:\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        action_history.append(action)\n",
    "        state_history.append(rgb2gray(state))\n",
    "        reward_history.append(reward)\n",
    "        action = agent.get_action(state)\n",
    "        if done:\n",
    "            print(\"Done\")\n",
    "            break\n",
    "        if agent.done:\n",
    "            print(\"Agent done\")\n",
    "            done = True\n",
    "            break\n",
    "\n",
    "    day_time = datetime.today().strftime(\"%m%d%y_%H%M%S\")\n",
    "    np.savez(f\".\\\\agent_recordings\\\\{agent.name}_{day_time}\", np.array(state_history), np.array(action_history), np.array(reward_history))\n",
    "    print(\"Recording saved\")\n",
    "    env.close()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Windows closed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cb7bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rec_state_history, rec_action_history = get_state_action_pairs(2)\n",
    "rec_action_history = eliminate_unnecessary_action(rec_action_history)\n",
    "agent = CNNAgent(model=None, state_history=rec_state_history, action_history=rec_action_history)\n",
    "hyperparameter_tuning(rec_state_history, rec_action_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56728210-9c95-4324-b778-24f15d242506",
   "metadata": {},
   "source": [
    "# 4. Proximal Policy Optimization using Stable Baselines 3\n",
    "After finding that most classification approaches failed to yield useful results, we decided to consult the literature for a more appropriate approach. Promimal Policy Optimization (https://arxiv.org/pdf/1707.06347.pdf) is a reinforcement learning approach developed by OpenAI in 2017. Since then it has become a standard approach for reinforcement learning tasks. After doing some research online, we were able to find a utility wrapper for the SuperMarioBros gym environment. This wrapper accesses the RAM of the NES simulator directly, exposing the location of Mario and the scene features directly. This makes it easier for PPO to learn a performant policy more quickly as it doesn't need to learn the mapping from the image space to semanticly meaningful features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78297ee1-9587-44e9-a065-9e822348eb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All necessary imports\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym \n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, RIGHT_ONLY \n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from gym_utils import SMBRamWrapper\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6539a40c-00d9-49ec-8c28-8dc747cec589",
   "metadata": {},
   "source": [
    "### Defining Callback and Helper Functions\n",
    "The logging callback class is used to populate the score/time_step lists for later display.  \n",
    "The linear schedule is derived from the stablebaselines documentation, and is considered a standard approach for PPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bfb10e-357f-412c-b8af-c21e3f39fed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "class SaveCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, env_wrap, starting_steps=0, verbose=1):\n",
    "        super(SaveCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.starting_steps = starting_steps\n",
    "        self.model_directory = \"./\"\n",
    "\n",
    "    def _init_callback(self):\n",
    "        self.env_wrap = self.model.get_env()\n",
    "        print(\"Initialized Callback\")\n",
    "        if not os.path.exists(self.model_directory):\n",
    "            os.mkdir(self.model_directory)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            self.model.save(self.model_directory + \"model_\" + str(self.n_calls))\n",
    "        return True\n",
    "    \n",
    "# Linear learning rate schedule\n",
    "# https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#learning-rate-schedule\n",
    "from typing import Callable\n",
    "\n",
    "def linear_schedule(initial_value: float) -> Callable[[float], float]:\n",
    "    \"\"\"\n",
    "    Linear learning rate schedule.\n",
    "\n",
    "    :param initial_value: Initial learning rate.\n",
    "    :return: schedule that computes\n",
    "      current learning rate depending on remaining progress\n",
    "    \"\"\"\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        \"\"\"\n",
    "        Progress will decrease from 1 (beginning) to 0.\n",
    "\n",
    "        :param progress_remaining:\n",
    "        :return: current learning rate\n",
    "        \"\"\"\n",
    "        return progress_remaining * initial_value\n",
    "\n",
    "    return func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb769a3-9544-49a4-a904-7c589c97543e",
   "metadata": {},
   "source": [
    "### Creating the environment\n",
    "The environment we are using is the classic super mario bros. There is a RAM access wrapper to reduce the state space for easier policy learning, we well as monitor/dummy vec which enable callback functionality that we are using to save off versions of the model and track performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68608b1-0435-47fe-85ca-9be64d75e781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment, this needs to be re-run if you call env.close()\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v3')\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "x0 = 0\n",
    "x1 = 16\n",
    "y0 = 0\n",
    "y1 = 13\n",
    "n_stack = 4\n",
    "n_skip = 4\n",
    "env_wrap = SMBRamWrapper(env, [x0, x1, y0, y1], n_stack=n_stack, n_skip=n_skip)\n",
    "env_wrap = Monitor(env_wrap)\n",
    "env_wrap = DummyVecEnv([lambda: env_wrap])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444688c3-0c6b-4aa7-82a6-2287674185c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training models on different hyperparameters\n",
    "batch_sizes = [16,32,64]\n",
    "n_epochs = [8,10,12]\n",
    "gammas = [0.8,0.9,0.99]\n",
    "\n",
    "models = []\n",
    "evals = []\n",
    "for batch_size in batch_sizes:\n",
    "    for n_epoch in n_epochs:\n",
    "        for gamma in gammas:\n",
    "            model = PPO('MlpPolicy', env_wrap, verbose=0, learning_rate=linear_schedule(3e-4), batch_size=batch_size,n_epochs=n_epoch,gamma=gamma) \n",
    "            model.learn(total_timesteps=1e3)\n",
    "            eval = evaluate_policy(model, env_wrap, n_eval_episodes=1, deterministic=True, render=False, return_episode_rewards=False)\n",
    "            models.append(model)\n",
    "            evals.append(eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafbba2b-5ba5-4cbd-8af5-669220b672eb",
   "metadata": {},
   "source": [
    "### Results of hyperparameter selection\n",
    "As is evident, the groups are largely split into >200 or <350 in terms of average reward per life. Inspection of the replays indicates that this is due to either dying to the first enemy, or jumping over and moving forward. We will select the model with the highest performance and move on to more rigorous training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8046e0d1-4d92-4954-97a7-4ae09007dcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average rewards of all the trained models\n",
    "plt.figure()\n",
    "evals = np.array(evals)\n",
    "evals = evals[:,0]\n",
    "plt.bar([i for i in range(len(evals))],evals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cbe38c-1287-4b98-9819-24f2e1e9e23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the combination of the best hyperparameters\n",
    "best_model_index = np.argmax(evals)\n",
    "best_model = models[best_model_index]\n",
    "best_batch_size = best_model.batch_size\n",
    "best_num_epochs = best_model.n_epochs\n",
    "best_gamma = best_model.gamma\n",
    "print(\"Best performance: \", max(evals))\n",
    "print(\"Best Batch Size: \",best_batch_size)\n",
    "print(\"Best Number of Epochs: \",best_num_epochs)\n",
    "print(\"Best Gamma: \", best_gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5862e3ab-04ec-4197-8a7b-550fce87f1ba",
   "metadata": {},
   "source": [
    "From the last line we have determined that the optimal batch_size is 16, n_epochs is 12, and gamma is 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618ed1e9-b06b-4d5c-a8d0-8a0e2676239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an optimal model and train it for a significant amount of time\n",
    "best_batch_size = 16\n",
    "best_num_epochs = 12\n",
    "best_gamma = 0.8\n",
    "\n",
    "\n",
    "del model\n",
    "model = PPO('MlpPolicy', env_wrap, verbose=0, learning_rate=linear_schedule(3e-4), batch_size=best_batch_size,n_epochs=best_num_epochs,gamma=best_gamma) \n",
    "callback = SaveCallback(check_freq=1e4, starting_steps=0, env_wrap = env_wrap)\n",
    "model.learn(total_timesteps=1e3, callback = callback)\n",
    "eval = evaluate_policy(model, env_wrap, n_eval_episodes=1, deterministic=True, render=False, return_episode_rewards=False)\n",
    "print(\"Performance: \", eval)\n",
    "model.save(\"best_model1e3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001532d4-0c4b-476b-97d3-c7213c61ae12",
   "metadata": {},
   "source": [
    "### Seeing our best performance\n",
    "Here we will run a simulation using the PPO agent to see what it has learned. A disclaimer, once the simulation is run the environemnt will close, necessitating a new one to be loaded if further notebook use is desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daca74b7-9c9f-4bb2-9884-3cd11479182b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model:\n",
    "    del model\n",
    "model = PPO('MlpPolicy', env_wrap, verbose=0, learning_rate=linear_schedule(3e-4), batch_size=best_batch_size,n_epochs=best_num_epochs,gamma=best_gamma) \n",
    "model.load(\"best_model1e6\")\n",
    "states = env_wrap.reset()\n",
    "done = False\n",
    "score = 0\n",
    "max_steps = 1e4\n",
    "while not done and max_steps > 0:\n",
    "    max_steps -=1\n",
    "    env_wrap.render()\n",
    "    action, _ = mod.predict(states, deterministic=True)\n",
    "    states, reward, done, info = env_wrap.step(action)\n",
    "    score += reward\n",
    "    time.sleep(0.01)\n",
    "print('Score: ', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5cdb7f-b6e8-409a-9763-9219199bb5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9137bd99-ec33-4147-94da-7204f272e516",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "All in all our experience demonstrates the difficulties of learning to play a game using classical machine learning approaches. We also demonstrate the power of new Reinforcement Learning approaches in exporing the state space and finding a performant policy. We also attemped oher approaches such as Behavioral Cloning, Adversarial Inverse Reinforcement Learning, and Generative Adversarial Imitation Learning, however the stable baselines implementations were not compatible with the gym verisons we are using. Further work should include integrating these other state of the art approaches into our work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "04e1ccc7392dfbed23a43c0e32571dfcfa1fbb6ca9607b30128bf025426f4a67"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
