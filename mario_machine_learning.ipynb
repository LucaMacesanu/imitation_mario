{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f00380ee-e1f5-4933-b477-aa22beae37e9",
   "metadata": {},
   "source": [
    "# Classification Based Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778c7809-0a65-4767-a59e-24a139de2bd8",
   "metadata": {},
   "source": [
    "# 1. KNN\n",
    "\n",
    "We used KNN as an example as to why our agent should not use classification methods using state-action pairsto solve the issue. Initially, KNN needs all of its moves to be scaled into a determined set. Since there is so much different terrain, it is harder for it to adapt once a new unseen terrain is added or when an object moves to a new location relative to the frame.\n",
    "\n",
    "Another issue for KNN is the curse of dimensionality. This dataset is composed of two numpy arrays: a 3D array that describes the environment and a 1D array that describes the action taken. In many cases in training, our players did drastically different actions at the same observation or the same action at a slightly different observation. Therefore, KNN has difficulties in finding similarities between records. This resulted in our KNN model taking vast amounts of time for training off of a singular isloated run and thus not finishing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b321f5-20c2-4598-b877-8324f1296921",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc1fbec4-e486-4699-8fec-79df63948353",
   "metadata": {},
   "source": [
    "# 2. Decision Trees\n",
    "We also used a decision tree to further demonstrate how classification using state-action pairs was inappropriate for our problem. Our data is filled with many occurrences of the same observation but slightly different actions in reaction to the same environment. \n",
    "In one run, the same person could jump at a frame earlier and thus not jump at the consequent frame or the same person could jump\n",
    "at the consequent frame and not do anything in the frame prior. This makes it difficult for the agent to draw similarities\n",
    "between runs, making it hard for the tree to fit an action to a specific state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ee5e5e-64b3-4829-a5a8-1e39edc5c4e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f4bafbb-8437-4b11-83ce-ab35f25aa9fa",
   "metadata": {},
   "source": [
    "# 3. ADABoosting\n",
    "We used ADABoosting as an example as to why our agent should not use classification methods using state-action pairs to play super mario. Though the accuracy of the boosting model is generally higher than techniques alone, in our case, since we are using ensembling in a supervised learning way based on the decision tree stump model, the ensemble booster model accuracy is ____. Since the goal of our classification is to classify in a way that allows the model to beat the super mario game, the application of ensembling wouldn't be the best as it would base the action it takes strictly on the states ran through by a mario play-through done by the user, which could be susceptible to faulty playthroughs. If we were to apply an RL agent on the other hand which creates policies on the premise of maximizing rewards as our model for ADABoosting, it would likely be a good approach as the accuracy would be increased substantially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53c5181-42c4-4498-aeb1-d0d7f1abb031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ba1e01-1651-4b39-a66d-47b2ac58f7d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56728210-9c95-4324-b778-24f15d242506",
   "metadata": {},
   "source": [
    "# 1. Proximal Policy Optimization using Stable Baselines 3\n",
    "Insert an explanation of PPO here, why we used it, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78297ee1-9587-44e9-a065-9e822348eb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All necessary imports\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym \n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, RIGHT_ONLY \n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from gym_utils import SMBRamWrapper\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbc04a9f-12b5-4316-b779-b0b577292836",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v3')\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be90062c-f7d7-4212-bcbf-e7a8e24ba1ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d68608b1-0435-47fe-85ca-9be64d75e781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup cropping size\n",
    "x0 = 0\n",
    "x1 = 16\n",
    "y0 = 0\n",
    "y1 = 13\n",
    "n_stack = 4\n",
    "n_skip = 4\n",
    "\n",
    "env_wrap = SMBRamWrapper(env, [x0, x1, y0, y1], n_stack=n_stack, n_skip=n_skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fc61738-1bb8-4cbe-8b87-6b0a708aaa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply other wrapper functions\n",
    "env_wrap = Monitor(env_wrap)  # for tensorboard log\n",
    "env_wrap = DummyVecEnv([lambda: env_wrap])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4bfb10e-357f-412c-b8af-c21e3f39fed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "\n",
    "# Save intermediate models\n",
    "# Copied from Nicholas Renotte's code\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, \n",
    "                 starting_steps=0, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "        self.starting_steps = starting_steps\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls + int(self.starting_steps)))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True\n",
    "    \n",
    "# Linear learning rate schedule\n",
    "# https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#learning-rate-schedule\n",
    "from typing import Callable\n",
    "\n",
    "def linear_schedule(initial_value: float) -> Callable[[float], float]:\n",
    "    \"\"\"\n",
    "    Linear learning rate schedule.\n",
    "\n",
    "    :param initial_value: Initial learning rate.\n",
    "    :return: schedule that computes\n",
    "      current learning rate depending on remaining progress\n",
    "    \"\"\"\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        \"\"\"\n",
    "        Progress will decrease from 1 (beginning) to 0.\n",
    "\n",
    "        :param progress_remaining:\n",
    "        :return: current learning rate\n",
    "        \"\"\"\n",
    "        return progress_remaining * initial_value\n",
    "\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64ccad8c-13c4-4b5a-b9c3-f17e42576588",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_time = datetime.today().strftime(\"%m%d%y_%H%M%S\")\n",
    "MODEL_DIR = './models/PPO_' + day_time\n",
    "LOG_DIR = './logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "444688c3-0c6b-4aa7-82a6-2287674185c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env_wrap, verbose=0, learning_rate=linear_schedule(3e-4), tensorboard_log=LOG_DIR) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74708140-9489-48ce-9e4c-0e85df03d267",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=1e5, starting_steps=0, save_path=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "618ed1e9-b06b-4d5c-a8d0-8a0e2676239b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucam\\miniconda3\\envs\\yumouwei\\lib\\site-packages\\gym_super_mario_bros\\smb_env.py:148: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 61.61 s\n"
     ]
    }
   ],
   "source": [
    "#Training the model\n",
    "t_start = time.time()\n",
    "model.learn(total_timesteps=10e3, callback=callback)\n",
    "t_elapsed = time.time() - t_start\n",
    "print('Wall time: {} s'.format(round(t_elapsed, 2)))\n",
    "# Save model\n",
    "model_path = os.path.join(MODEL_DIR)\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96e0337e-915b-4bfd-bf5a-66e8001cc6e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(232.0, 0.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "evaluate_policy(model, env_wrap, n_eval_episodes=1, deterministic=True, render=False, return_episode_rewards=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daca74b7-9c9f-4bb2-9884-3cd11479182b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucam\\miniconda3\\envs\\yumouwei\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] Cannot change thread mode after it is set\n",
      "  warnings.warn(str(err))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:[232.]\n"
     ]
    }
   ],
   "source": [
    "episode = 1\n",
    "\n",
    "for episode in range(1, episode+1):\n",
    "    states = env_wrap.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env_wrap.render()\n",
    "        action, _ = model.predict(states, deterministic=True)\n",
    "        states, reward, done, info = env_wrap.step(action)\n",
    "        score += reward\n",
    "        time.sleep(0.01)\n",
    "    print('Episode:{} Score:{}'.format(episode, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb5cdb7f-b6e8-409a-9763-9219199bb5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
