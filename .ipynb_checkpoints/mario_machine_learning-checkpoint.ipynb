{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f00380ee-e1f5-4933-b477-aa22beae37e9",
   "metadata": {},
   "source": [
    "# Classification Based Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778c7809-0a65-4767-a59e-24a139de2bd8",
   "metadata": {},
   "source": [
    "# 1. KNN\n",
    "\n",
    "We used KNN as an example as to why our agent should not use classification methods using state-action pairsto solve the issue. Initially, KNN needs all of its moves to be scaled into a determined set. Since there is so much different terrain, it is harder for it to adapt once a new unseen terrain is added or when an object moves to a new location relative to the frame.\n",
    "\n",
    "Another issue for KNN is the curse of dimensionality. This dataset is composed of two numpy arrays: a 3D array that describes the environment and a 1D array that describes the action taken. In many cases in training, our players did drastically different actions at the same observation or the same action at a slightly different observation. Therefore, KNN has difficulties in finding similarities between records. This resulted in our KNN model taking vast amounts of time for training off of a singular isloated run and thus not finishing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b321f5-20c2-4598-b877-8324f1296921",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc1fbec4-e486-4699-8fec-79df63948353",
   "metadata": {},
   "source": [
    "# 2. Decision Trees\n",
    "We also used a decision tree to further demonstrate how classification using state-action pairs was inappropriate for our problem. Our data is filled with many occurrences of the same observation but slightly different actions in reaction to the same environment. \n",
    "In one run, the same person could jump at a frame earlier and thus not jump at the consequent frame or the same person could jump\n",
    "at the consequent frame and not do anything in the frame prior. This makes it difficult for the agent to draw similarities\n",
    "between runs, making it hard for the tree to fit an action to a specific state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ee5e5e-64b3-4829-a5a8-1e39edc5c4e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f4bafbb-8437-4b11-83ce-ab35f25aa9fa",
   "metadata": {},
   "source": [
    "# 3. ADABoosting\n",
    "We used ADABoosting as an example as to why our agent should not use classification methods using state-action pairs to play super mario. Though the accuracy of the boosting model is generally higher than techniques alone, in our case, since we are using ensembling in a supervised learning way based on the decision tree stump model, the ensemble booster model accuracy is ____. Since the goal of our classification is to classify in a way that allows the model to beat the super mario game, the application of ensembling wouldn't be the best as it would base the action it takes strictly on the states ran through by a mario play-through done by the user, which could be susceptible to faulty playthroughs. If we were to apply an RL agent on the other hand which creates policies on the premise of maximizing rewards as our model for ADABoosting, it would likely be a good approach as the accuracy would be increased substantially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53c5181-42c4-4498-aeb1-d0d7f1abb031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9dcd9ab3",
   "metadata": {},
   "source": [
    "# 4. CNN\n",
    "\n",
    "Process: Our process involved splitting the dataset into training and validation sets using a 80-20 ratio. We tested every combination of kernel size and activation function for 4 epochs. Once we decided not to vary our batch size we kept it constant at 32. We then recorded the training histories which included the training and validation accuracies for each combination of hyperparameters.\n",
    "\n",
    "Conclusion: Based on hyperparameter tuning we found that the combination of a kernel size with (3,3) and the tanh activation function seemed to be the most effective as evidenced by the training and validation accuracy.\n",
    "\n",
    "Further steps: In order to further tune our hyperparameters to make more accurate predictions we could vary the number of our convolutional layers. Since increasing the number of convolutional layers may allow our model to distinguish between more complex representations, we may be able to improve our performance. We could also test different learning rates to optimize convergence.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b382e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_super_mario_bros\n",
    "import gym\n",
    "import nes_py\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from nes_py.app.play_human import play_human\n",
    "from gym.wrappers.gray_scale_observation import GrayScaleObservation\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import pickle\n",
    "from playback import get_state_action_pairs \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79446eca",
   "metadata": {},
   "source": [
    "The below code converts our game space from rgb to grayscale. Eliminate unecessary action remaps our actionspace from 0->7 as this is what our action input data is looking for. Each number responds to a specific action such as 128 occurs when the user click d and it causes Mario to move forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd15d606",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rgb2gray(rgb):\n",
    "    r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n",
    "    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "    return gray\n",
    "\n",
    "def eliminate_unnecessary_action(data):\n",
    "    action_space = data\n",
    "    action_map = {\n",
    "        64: 0,\n",
    "        65: 1,\n",
    "        66: 2,\n",
    "        67: 3,\n",
    "        128: 4,\n",
    "        129: 5,\n",
    "        130: 6,\n",
    "        131: 7\n",
    "    }\n",
    "    \n",
    "    action_space = np.array([action_map.get(action, 0) for action in action_space])\n",
    "    return action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0275a2",
   "metadata": {},
   "source": [
    "The code below defines a specific CNN architecture. It is using TensorFlow's keras library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c3d16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CNN model with specific layers, some values given from CNN tutorial with tensorflow\n",
    "def create_cnn(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6820097",
   "metadata": {},
   "source": [
    "Kernel Sizes: We tested three different kernel sizes ((3,3),(5,5),(7,7)). As a smaller kernel sizes focus on smaller regions of our input image, we expected the larger kernel sizes to be more effective as they would capture a wider range of information from our input image.\n",
    "\n",
    "Activation Functions: We tested three different activation functions (Rectified Linear Unit, Hyperbolic Tangent, and Sigmoid).\n",
    "As relu (Rectified Linear Unit), is known for its effectiveness with regards to deep learning models, we expected it to perform the best.\n",
    "\n",
    "Note: Tried batch sizes but didn't feel it had a significant enough change in our accuracy. Now we set our batch size to 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a4300b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning(state_history, action_history):\n",
    "    kernel_sizes = [(3, 3), (5, 5),(7,7)]\n",
    "    activation_functions = ['relu','tanh','sigmoid']\n",
    "    #batch_sizes = [32, 64, 128]\n",
    "\n",
    "    training_histories = []\n",
    "    accuracies = []\n",
    "\n",
    "    for kernel_size in kernel_sizes:\n",
    "        for activation_function in activation_functions:\n",
    "            print(f\"training with kernel size: {kernel_size} and activation function: {activation_function}\")\n",
    "            train_a, test_a, train_b, test_b = train_test_split(state_history, action_history, test_size=0.2, random_state=42)\n",
    "            train_a = np.expand_dims(train_a, axis=-1)\n",
    "            test_a = np.expand_dims(test_a, axis=-1)\n",
    "            \n",
    "            train_a = train_a / 255.0\n",
    "            test_a = test_a / 255.0\n",
    "\n",
    "            model = create_cnn(input_shape=(state_history.shape[1], state_history.shape[2], 1), num_classes=np.max(action_history) + 1)\n",
    "            model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "            history = model.fit(train_a, train_b, epochs=4, batch_size=32, validation_data=(test_a, test_b))\n",
    "\n",
    "            training_histories.append(history)\n",
    "            accuracies.append(history.history['val_accuracy'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16223893",
   "metadata": {},
   "source": [
    "The code below graphs both the training accuracy and the validation accuracy over a specified amount of epochs. The graph line colors correspond to the specific combination of kernel sizes and and activation functions. The same combination color is used for the training accuracy and the validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25fe9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "    plt.figure(figsize=(10, 12))\n",
    "    colors = plt.cm.tab10.colors[:len(kernel_sizes) * len(activation_functions)]\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    for i, kernel_size in enumerate(kernel_sizes):\n",
    "        for j, activation_function in enumerate(activation_functions):\n",
    "            plt.plot(range(1, 5), training_histories[i*len(activation_functions)+j].history['accuracy'], label=f'kernel size:{kernel_size}, activation:{activation_function}')\n",
    "\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('Training Accuracy')\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15),ncol=3,fontsize='small')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    for i, kernel_size in enumerate(kernel_sizes):\n",
    "        for j, activation_function in enumerate(activation_functions):\n",
    "            color = colors[i*len(activation_functions)+j\n",
    "            plt.plot(range(1, 5), training_histories[i*len(activation_functions)+j].history['val_accuracy'], label=f'kernel size: {kernel_size}, activation:{activation_function}', color=color)\n",
    "\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('validation accuracy')\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=3, fontsize='small')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Validation Accuracies:\")\n",
    "    for i, kernel_size in enumerate(kernel_sizes):\n",
    "        for j, activation_function in enumerate(activation_functions):\n",
    "            print(f'kernel size:{kernel_size}, activation:{activation_function}: {accuracies[i*len(activation_functions)+j]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492c49bf",
   "metadata": {},
   "source": [
    "Define the CNN Agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0de4aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNAgent:\n",
    "    def __init__(self, model=None, state_history=None, action_history=None):\n",
    "        self.done = False\n",
    "        self.name = \"cnn_agent\"\n",
    "        if model is None:\n",
    "            num_classes = np.max(action_history) + 1\n",
    "            self.model = create_cnn(input_shape=(state_history.shape[1], state_history.shape[2], 1), num_classes=num_classes)\n",
    "        else:\n",
    "            self.model = model\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        state = np.expand_dims(state, axis=-1)\n",
    "        return np.argmax(self.model.predict(state))\n",
    "        \n",
    "    def train(self, state_history, action_history):\n",
    "\n",
    "        print(\"Training...\")\n",
    "        train_a, test_a, train_b, test_b = train_test_split(state_history, action_history, test_size=0.2, random_state=42)\n",
    "        train_a = np.expand_dims(train_a, axis=-1)\n",
    "        test_a = np.expand_dims(test_a, axis=-1)\n",
    "        train_a = train_a / 255.0\n",
    "        test_a = test_a / 255.0\n",
    "        self.model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        history = self.model.fit(train_a, train_b, epochs=1, validation_data=(test_a, test_b))\n",
    "        self.model.save(\"models/cnn_model.h5\")\n",
    "        print(\"Training complete.\")\n",
    "\n",
    "        #Graph validation and training accuracy as epochs increase\n",
    "        plt.plot(history.history['accuracy'], label='accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.show()\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311b15c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(agent):\n",
    "\n",
    "    env = GrayScaleObservation(gym_super_mario_bros.make(\"SuperMarioBros-v3\"))\n",
    "    env.reset()\n",
    "\n",
    "    action_history = []\n",
    "    state_history = []\n",
    "    reward_history = []\n",
    "\n",
    "    action = [0]\n",
    "    done = False\n",
    "    while not done:\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        action_history.append(action)\n",
    "        state_history.append(rgb2gray(state))\n",
    "        reward_history.append(reward)\n",
    "        action = agent.get_action(state)\n",
    "        if done:\n",
    "            print(\"Done\")\n",
    "            break\n",
    "        if agent.done:\n",
    "            print(\"Agent done\")\n",
    "            done = True\n",
    "            break\n",
    "\n",
    "    day_time = datetime.today().strftime(\"%m%d%y_%H%M%S\")\n",
    "    np.savez(f\".\\\\agent_recordings\\\\{agent.name}_{day_time}\", np.array(state_history), np.array(action_history), np.array(reward_history))\n",
    "    print(\"Recording saved\")\n",
    "    env.close()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Windows closed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cb7bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rec_state_history, rec_action_history = get_state_action_pairs(2)\n",
    "rec_action_history = eliminate_unnecessary_action(rec_action_history)\n",
    "agent = CNNAgent(model=None, state_history=rec_state_history, action_history=rec_action_history)\n",
    "hyperparameter_tuning(rec_state_history, rec_action_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29746f3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56728210-9c95-4324-b778-24f15d242506",
   "metadata": {},
   "source": [
    "# 4. Proximal Policy Optimization using Stable Baselines 3\n",
    "After finding that most classification approaches failed to yield useful results, we decided to consult the literature for a more appropriate approach. Promimal Policy Optimization (https://arxiv.org/pdf/1707.06347.pdf) is a reinforcement learning approach developed by OpenAI in 2017. Since then it has become a standard approach for reinforcement learning tasks. After doing some research online, we were able to find a utility wrapper for the SuperMarioBros gym environment. This wrapper accesses the RAM of the NES simulator directly, exposing the location of Mario and the scene features directly. This makes it easier for PPO to learn a performant policy more quickly as it doesn't need to learn the mapping from the image space to semanticly meaningful features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78297ee1-9587-44e9-a065-9e822348eb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All necessary imports\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym \n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, RIGHT_ONLY \n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from gym_utils import SMBRamWrapper\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6539a40c-00d9-49ec-8c28-8dc747cec589",
   "metadata": {},
   "source": [
    "### Defining Callback and Helper Functions\n",
    "The logging callback class is used to populate the score/time_step lists for later display.  \n",
    "The linear schedule is derived from the stablebaselines documentation, and is considered a standard approach for PPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4bfb10e-357f-412c-b8af-c21e3f39fed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, env_wrap, starting_steps=0, verbose=1):\n",
    "        super(EvaluationCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.starting_steps = starting_steps\n",
    "        self.evaluations = []\n",
    "\n",
    "    def _init_callback(self):\n",
    "        self.env_wrap = self.model.get_env()\n",
    "        print(\"Initialized Callback\")\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            eval = evaluate_policy(self.model, self.env_wrap, n_eval_episodes=1, deterministic=True, render=False, return_episode_rewards=False)\n",
    "            self.evaluations.append(eval[0])\n",
    "        return True\n",
    "    \n",
    "# Linear learning rate schedule\n",
    "# https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#learning-rate-schedule\n",
    "from typing import Callable\n",
    "\n",
    "def linear_schedule(initial_value: float) -> Callable[[float], float]:\n",
    "    \"\"\"\n",
    "    Linear learning rate schedule.\n",
    "\n",
    "    :param initial_value: Initial learning rate.\n",
    "    :return: schedule that computes\n",
    "      current learning rate depending on remaining progress\n",
    "    \"\"\"\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        \"\"\"\n",
    "        Progress will decrease from 1 (beginning) to 0.\n",
    "\n",
    "        :param progress_remaining:\n",
    "        :return: current learning rate\n",
    "        \"\"\"\n",
    "        return progress_remaining * initial_value\n",
    "\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d68608b1-0435-47fe-85ca-9be64d75e781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v3')\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "x0 = 0\n",
    "x1 = 16\n",
    "y0 = 0\n",
    "y1 = 13\n",
    "n_stack = 4\n",
    "n_skip = 4\n",
    "env_wrap = SMBRamWrapper(env, [x0, x1, y0, y1], n_stack=n_stack, n_skip=n_skip)\n",
    "env_wrap = Monitor(env_wrap)\n",
    "env_wrap = DummyVecEnv([lambda: env_wrap])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64ccad8c-13c4-4b5a-b9c3-f17e42576588",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_time = datetime.today().strftime(\"%m%d%y_%H%M%S\")\n",
    "MODEL_DIR = './models/PPO_' + day_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "444688c3-0c6b-4aa7-82a6-2287674185c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training bs:  16  n_e:  8  gamma:  0.8\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "learn() got an unexpected keyword argument 'progress_bar'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining bs: \u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_size, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m n_e: \u001b[39m\u001b[38;5;124m\"\u001b[39m, n_epoch, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m gamma: \u001b[39m\u001b[38;5;124m\"\u001b[39m, gamma)\n\u001b[0;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m'\u001b[39m, env_wrap, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39mlinear_schedule(\u001b[38;5;241m3e-4\u001b[39m), batch_size\u001b[38;5;241m=\u001b[39mbatch_size,n_epochs\u001b[38;5;241m=\u001b[39mn_epoch,gamma\u001b[38;5;241m=\u001b[39mgamma) \n\u001b[1;32m---> 13\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10e2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28meval\u001b[39m \u001b[38;5;241m=\u001b[39m evaluate_policy(model, env_wrap, n_eval_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, render\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, return_episode_rewards\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     15\u001b[0m models\u001b[38;5;241m.\u001b[39mappend(model)\n",
      "\u001b[1;31mTypeError\u001b[0m: learn() got an unexpected keyword argument 'progress_bar'"
     ]
    }
   ],
   "source": [
    "# Hyperparameters to tune on\n",
    "batch_sizes = [16,32,64]\n",
    "n_epochs = [8,10,12]\n",
    "gammas = [0.8,0.9,0.99]\n",
    "\n",
    "models = []\n",
    "evals = []\n",
    "for batch_size in batch_sizes:\n",
    "    for n_epoch in n_epochs:\n",
    "        for gamma in gammas:\n",
    "            print(\"Training bs: \", batch_size, \" n_e: \", n_epoch, \" gamma: \", gamma)\n",
    "            model = PPO('MlpPolicy', env_wrap, verbose=0, learning_rate=linear_schedule(3e-4), batch_size=batch_size,n_epochs=n_epoch,gamma=gamma) \n",
    "            model.learn(total_timesteps=1e2)\n",
    "            eval = evaluate_policy(model, env_wrap, n_eval_episodes=1, deterministic=True, render=False, return_episode_rewards=False)\n",
    "            models.append(model)\n",
    "            evals.append(eval)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74708140-9489-48ce-9e4c-0e85df03d267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618ed1e9-b06b-4d5c-a8d0-8a0e2676239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Training all of the models\n",
    "# t_start = time.time()\n",
    "# model.learn(total_timesteps=10e3, callback=callback,progress_bar=True)\n",
    "# t_elapsed = time.time() - t_start\n",
    "# print('Wall time: {} s'.format(round(t_elapsed, 2)))\n",
    "# # Save model\n",
    "# model_path = os.path.join(MODEL_DIR)\n",
    "# model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e0337e-915b-4bfd-bf5a-66e8001cc6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "evaluate_policy(model, env_wrap, n_eval_episodes=1, deterministic=True, render=False, return_episode_rewards=False)\n",
    "plt.figure()\n",
    "plt.plot(callback.evaluations)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daca74b7-9c9f-4bb2-9884-3cd11479182b",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = 1\n",
    "\n",
    "for episode in range(1, episode+1):\n",
    "    states = env_wrap.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env_wrap.render()\n",
    "        action, _ = model.predict(states, deterministic=True)\n",
    "        states, reward, done, info = env_wrap.step(action)\n",
    "        score += reward\n",
    "        time.sleep(0.01)\n",
    "    print('Episode:{} Score:{}'.format(episode, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5cdb7f-b6e8-409a-9763-9219199bb5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ee92a4-1425-466d-ae9f-70090c3c8ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback.evaluations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
